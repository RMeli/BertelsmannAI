{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we want to implement a word-level LSTM network in order to automatically annotate positive and negative IMDB movies reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe96c1579b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing is often a big chunk of every data science, machine learning and deep learning project. Here we want to load IMDB movie reviews and remove very long and very short reviews and encode them in a suitable way for our LSTM network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the reviews and labels are contained in two separate text files, that can be loaded easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews:\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      "story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turn\n",
      "\n",
      "Labels:\n",
      "positive\n",
      "negative\n",
      "positiv\n"
     ]
    }
   ],
   "source": [
    "reviewsfname = \"data/IMDB-Reviews.txt\"\n",
    "labelsfname = \"data/IMDB-Labels.txt\"\n",
    "\n",
    "# Load revioews\n",
    "with open(reviewsfname, \"r\") as f:\n",
    "    reviews = f.read()\n",
    "    \n",
    "# Load labels\n",
    "with open(labelsfname, \"r\") as f:\n",
    "    labels = f.read()\n",
    "    \n",
    "print(f\"Reviews:\\n{reviews[:1000]}\")\n",
    "print(f\"\\nLabels:\\n{labels[:25]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews can be very messy: they can contain capital letters, lower case letters, all sort of punctiation, etc. In order to make them more uniform we want to remove all punctuation and convert them to lowercase letters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "# Look at punctuation\n",
    "# String containing all puncutation symbols.\n",
    "print(f\"Punctuation: {punctuation}\")\n",
    "\n",
    "# Convert reviews to lowercase\n",
    "reviews = reviews.lower()\n",
    "labels = labels.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "reviews = \"\".join([char for char in reviews if char not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned reviews (lowercase, no punctuation) contain only words, therefore in order to get a list of all words we can split the review in all the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the']\n"
     ]
    }
   ],
   "source": [
    "allwords = reviews.split()\n",
    "\n",
    "print(allwords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different reviews and labels are separated by newline characters in our dataset. Therefore we can easily split the full text of reviews and labels into a list of reviews and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.split(\"\\n\")\n",
    "labels = labels.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell high is a cartoon comedy  it ran at the same time as some other programs about school life  such as  teachers   my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers   the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students  when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled          at           high  a classic line inspector i  m here to sack one of your teachers  student welcome to bromwell high  i expect that many adults of my age think that bromwell high is far fetched  what a pity that it isn  t   ', 'story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly   ']\n",
      "['positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Check that we have the same number of reviews and labels\n",
    "assert len(reviews) == len(labels)\n",
    "\n",
    "print(reviews[:2])\n",
    "print(labels[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words must be encoded in order to be passed to the LSTM network. An easy encoding is to map every unique word to a number. We can start by getting a set of unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 74072\n",
      "\n",
      "['a', 'aa', 'aaa', 'aaaaaaah', 'aaaaah', 'aaaaatch', 'aaaahhhhhhh', 'aaaand', 'aaaarrgh', 'aaah', 'aaargh', 'aaaugh', 'aaawwwwnnn', 'aachen', 'aada', 'aadha', 'aag', 'aage', 'aaghh', 'aah', 'aahhh', 'aaip', 'aaja', 'aakash', 'aaker', 'aakrosh', 'aaliyah', 'aames', 'aamir', 'aan']\n"
     ]
    }
   ],
   "source": [
    "words = sorted(set(allwords))\n",
    "n_words = len(words)\n",
    "\n",
    "print(f\"Number of unique words: {n_words}\")\n",
    "print(f\"\\n{words[:30]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the words we can map them to an integer. The first word in `words` is mapped to `1`, the second word is mapped to `2` and so on. We start the mapping at `1` because revews have different lengths (different number or words) and therefore we keep the value `0` for padding short reviews to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word to int mapping with 1 as first index\n",
    "word2int = {word : i for i, word in enumerate(words, 1)}\n",
    "\n",
    "# Create inverse int to word mapping\n",
    "int2word = {i : word for word, i in word2int.items()}\n",
    "\n",
    "# Check conversion\n",
    "assert len(word2int) == len(int2word)\n",
    "for word in words:\n",
    "    assert word == int2word[word2int[word]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a mapping between words and integers, we can encode all the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviw 1:\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon']\n",
      "\n",
      "Encoded Review 1:\n",
      "[8210, 29951, 33665, 1, 9819]\n",
      "\n",
      "Encoding:\n",
      "\n",
      "bromwell     => 8210\n",
      "high         => 29951\n",
      "is           => 33665\n",
      "a            => 1\n",
      "cartoon      => 9819\n",
      "\n",
      "Encoded Revies Shape: (25001,)\n"
     ]
    }
   ],
   "source": [
    "encodedreviews = []\n",
    "for review in reviews:\n",
    "    encodedreviews.append([word2int[word] for word in review.split()])\n",
    "\n",
    "# Visualize and check encoding\n",
    "print(f\"Reviw 1:\\n{reviews[0].split()[:5]}\")\n",
    "print(f\"\\nEncoded Review 1:\\n{encodedreviews[0][:5]}\")\n",
    "print(f\"\\nEncoding:\\n\")\n",
    "for word in reviews[0].split()[:5]:\n",
    "    print(f\"{word:12} => {word2int[word]}\")\n",
    "    \n",
    "encodedreviews = np.array(encodedreviews)\n",
    "print(f\"\\nEncoded Revies Shape: {encodedreviews.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews are labelles as `negative` or `positive`. This is a binary classification that can be easily encoded by `0` and `1`. Therefore we can easily encode all the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "['positive', 'negative', 'positive', 'negative', 'positive']\n",
      "\n",
      "Encoded Labels:\n",
      "[1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "encodedlabels = []\n",
    "for label in labels:\n",
    "    encodedlabels.append(0 if label == \"negative\" else 1)\n",
    "    \n",
    "# Visualize and check labels\n",
    "print(f\"Labels:\\n{labels[:5]}\")\n",
    "print(f\"\\nEncoded Labels:\\n{encodedlabels[:5]}\")\n",
    "\n",
    "# Transform labels to numpy array\n",
    "encodedlabels = np.array(encodedlabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very Short and Very Long Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews have very different lengths and therefore there are very short reviews and very long ones. Let's have a look at the review length distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Review Length: 0\n",
      "Maximum Review Length: 2514\n",
      "Average Review Length: 240.798\n",
      "Review Length STD: 179.017\n"
     ]
    }
   ],
   "source": [
    "reviewlenghts = [len(review) for review in encodedreviews]\n",
    "\n",
    "reviewlenghts = np.array(reviewlenghts)\n",
    "\n",
    "print(f\"Minimum Review Length: {np.min(reviewlenghts)}\")\n",
    "print(f\"Maximum Review Length: {np.max(reviewlenghts)}\")\n",
    "print(f\"Average Review Length: {np.mean(reviewlenghts):.3f}\")\n",
    "print(f\"Review Length STD: {np.std(reviewlenghts):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(reviewlenghts, bins=len(reviewlenghts)//100)\n",
    "plt.axvline(np.mean(reviewlenghts), color=\"orange\", )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily print out the very short reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 2137: this movie is terrible but it has some good effects   \n",
      "\n",
      "Review 4615: ming the merciless does a little bardwork and a movie most foul   \n",
      "\n",
      "Review 6267: comment this movie is impossible  is terrible  very improbable  bad interpretation e direction  not look       \n",
      "\n",
      "Review 7959: i wouldn  t rent this one even on dollar rental night   \n",
      "\n",
      "Review 8709: you  d better choose paul verhoeven  s even if you have watched it   \n",
      "\n",
      "Review 12816: adrian pasdar is excellent is this film  he makes a fascinating woman   \n",
      "\n",
      "Review 18873: no comment  stupid movie  acting average or worse    screenplay  no sense at all    skip it   \n",
      "\n",
      "Review 22219: long  boring  blasphemous  never have i been so glad to see ending credits roll   \n",
      "\n",
      "Review 25000: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "idxs = np.where(reviewlenghts <= 15)[0]\n",
    "\n",
    "for idx in idxs:\n",
    "    print(f\"Review {idx}: {reviews[idx]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that they make a lot of sense apart from the last one which is empty. Since it's the last one it probably comes from a spurious newline character at the end of the file. We can easily remove all reviews shorter than one word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Reviews Shape: (25000,)\n",
      "Encoded Labels Shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "encodedreviews = encodedreviews[reviewlenghts > 0] \n",
    "encodedlabels = encodedlabels[reviewlenghts > 0]\n",
    "\n",
    "print(f\"Encoded Reviews Shape: {encodedreviews.shape}\")\n",
    "print(f\"Encoded Labels Shape: {encodedlabels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncation and Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long reviews can be problematic since they will slow down the training process. Therefore we decide a maximum review length, `len_sequence` and truncate the long reviews to this length. Something close to the average review length should be enough. Since we want out output to have a fixed size, we also need to pad (with `0`) the shorter sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ... 33768 33726 64394]\n",
      " [    0     0     0 ...  5272 57783  8031]\n",
      " [30468 46281 30894 ...  7193 41074 46549]\n",
      " ...\n",
      " [    0     0     0 ... 15007 60837  4144]\n",
      " [    0     0     0 ... 46549 71568 25211]\n",
      " [    0     0     0 ... 65773 33665 47191]]\n"
     ]
    }
   ],
   "source": [
    "len_sequence = 256 # Total number of word in a review\n",
    "\n",
    "paddedreviews = np.zeros((encodedreviews.size, len_sequence), dtype=int)\n",
    "\n",
    "for i, review in enumerate(encodedreviews):\n",
    "    paddedreviews[i,-len(review):] = review[:len_sequence]\n",
    "    \n",
    "print(paddedreviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep track of the training process and evaluate the final model, we need to split our dataset into three sets: training, validation and test. This can be performed easily with the `train_test_split` function of [Scikit-learn](Scikit-learn). We keep 80% of the original dataset for training, 10% for validation and 10% for test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews Train: (20000, 256)\n",
      "Labels Train: (20000,)\n"
     ]
    }
   ],
   "source": [
    "# Split 80% of the dataset for training\n",
    "reviews_train, reviews_vt, labels_train, labels_vt = train_test_split(paddedreviews, encodedlabels, test_size=0.2)\n",
    "print(f\"Reviews Train: {reviews_train.shape}\\nLabels Train: {labels_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews Valid: (2500, 256)\n",
      "Labels Valid: (2500,)\n",
      "Reviews Test: (2500, 256)\n",
      "Labels Test: (2500,)\n"
     ]
    }
   ],
   "source": [
    "# Split the reminder 20% for validation and test (50%/50%)\n",
    "reviews_valid, reviews_test, labels_valid, labels_test = train_test_split(reviews_vt, labels_vt, test_size=0.5)\n",
    "print(f\"Reviews Valid: {reviews_valid.shape}\\nLabels Valid: {labels_valid.shape}\")\n",
    "print(f\"Reviews Test: {reviews_test.shape}\\nLabels Test: {labels_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders and Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset split in train, validation and test set we can build a `TensorDataset` (i.e. put together revies and their labels) and the corresponding `DataLoader`. We drop the last batch with `drop_last=True` so that all batches will have the same dimension (for easier debugging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TensorDataset(torch.from_numpy(reviews_train), torch.from_numpy(labels_train))\n",
    "validset = TensorDataset(torch.from_numpy(reviews_valid), torch.from_numpy(labels_valid))\n",
    "testset = TensorDataset(torch.from_numpy(reviews_test), torch.from_numpy(labels_test))\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "validloader = DataLoader(validset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "testloader = DataLoader(testset, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at one batch to check that everthing is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Shape: torch.Size([64, 256])\n",
      "Features:\n",
      "tensor([[    0,     0,     0,  ..., 25370, 71489, 33768],\n",
      "        [31424, 55987,     1,  ...,  7681,  7681, 49515],\n",
      "        [31424, 26653, 65773,  ...,  2477, 65543, 52123],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 10351, 45177, 33768],\n",
      "        [    0,     0,     0,  ..., 29251, 29037, 63296],\n",
      "        [61400, 38692, 65543,  ...,  4002, 45809, 65543]])\n",
      "Labels Shape: torch.Size([64])\n",
      "Labels:\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "trainiter = iter(trainloader)\n",
    "\n",
    "features, labels = next(trainiter)\n",
    "\n",
    "print(f\"Features Shape: {features.shape}\")\n",
    "print(f\"Features:\\n{features}\")\n",
    "print(f\"Labels Shape: {labels.shape}\")\n",
    "print(f\"Labels:\\n{labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM architecture is pretty much standard. We use a `nn.LSTM` module with dropout and a fully connected layer applied to the output. For a character-level RNN the input was one-hot-encoded since the number of different characters is limited. Unfortunately, encoding by words results in a large amount of different words that would be extremely inefficient to one-hot encode. In order to avoid one-hot encoding of several thousands word we can use a `nn.Embedding` (see [Word Embeddings: Encoding Lexical Semantics](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html?highlight=embedding) for more informations). An embedding layer allows to have a  dense encoding of words in a lower-dimensional space (compared to the very sparse one-hot encoding) and also take into account for semantic similarity between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 n_words, # Number of words \n",
    "                 s_embedding, # Embedding size\n",
    "                 s_hidden, # Hidden layer size\n",
    "                 n_layers, # Number of LSTM layers\n",
    "                 dropp=0.5 # Dropout probability\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        n_words += 1 # Add one word for 0 padding\n",
    "        \n",
    "        self.s_hidden = s_hidden\n",
    "        \n",
    "        # Embedding layer to avoid one-hot-encoding all words\n",
    "        # Reduce n_words to s_embedding for LSTM input\n",
    "        self.embedding = nn.Embedding(n_words, s_embedding)\n",
    "        \n",
    "        # Define LSTM layer with embedding input\n",
    "        self.lstm = nn.LSTM(s_embedding, s_hidden, n_layers, dropout=dropp, batch_first=True)\n",
    "        \n",
    "        # Define dropout layer between LSTM and FC\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "        # Fully connected layer with one output\n",
    "        # One ouptut for binary classification (positive/negative)\n",
    "        self.fc = nn.Linear(s_hidden, 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Perform word embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Propagate through LSTM\n",
    "        # Update hidden state\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Reshape for FC layer\n",
    "        x = x.reshape(-1, self.s_hidden)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connecred layer\n",
    "        # Compress to a single output\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Apply sigmoid function\n",
    "        # Returns values between 0 and 1\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        # Respahe according to batch size\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        \n",
    "        # Get last batch\n",
    "        x = x[:, -1]\n",
    "        \n",
    "        return x, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready for training. We can select hyperparameters for our model, instanciate it and move it to the device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 500)\n",
      "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "s_embedding = 500\n",
    "s_hidden = 256\n",
    "n_layers = 2\n",
    "\n",
    "model = SentimentRNN(n_words, s_embedding, s_hidden, n_layers)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification problem (positive/negative review) we can use the binary cross-entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.005\n",
    "\n",
    "# Binary Cross-Entropy Loss for binary classification\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation are implemended as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 0/5\n",
      "    Step: 156\n",
      "    Validation Loss: 0.97895\n",
      "--- Epoch 0/5\n",
      "    Step: 312\n",
      "    Validation Loss: 0.99879\n",
      "--- Epoch 1/5\n",
      "    Step: 468\n",
      "    Validation Loss: 1.83703\n",
      "--- Epoch 1/5\n",
      "    Step: 624\n",
      "    Validation Loss: 1.77770\n",
      "--- Epoch 2/5\n",
      "    Step: 780\n",
      "    Validation Loss: 1.75985\n",
      "--- Epoch 2/5\n",
      "    Step: 936\n",
      "    Validation Loss: 1.71410\n",
      "--- Epoch 3/5\n",
      "    Step: 1092\n",
      "    Validation Loss: 2.28626\n",
      "--- Epoch 3/5\n",
      "    Step: 1248\n",
      "    Validation Loss: 2.29612\n",
      "--- Epoch 4/5\n",
      "    Step: 1404\n",
      "    Validation Loss: 2.86544\n",
      "--- Epoch 4/5\n",
      "    Step: 1560\n",
      "    Validation Loss: 2.77071\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "print_every = len(trainloader) // 2\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "\n",
    "steps = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Reset hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    for review, label in trainloader:\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        # Reset optimizer gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move review and label to device\n",
    "        review, label = review.to(device), label.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, hidden = model(review, hidden)\n",
    "        \n",
    "        # Detach hiddent state from computational graph\n",
    "        # Avoid backpropagation through the entire history\n",
    "        hidden = tuple([h.detach() for h in hidden])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_function(output.squeeze(), label.float())\n",
    "        \n",
    "        # Perform backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients norm\n",
    "        # Prevents the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        \n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            \n",
    "            # Evaluation mode\n",
    "            # Turn off dropout\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                \n",
    "                # Reset hidden state for validation\n",
    "                hidden_v = None\n",
    "                \n",
    "                # Validation loss\n",
    "                loss_v = 0\n",
    "                \n",
    "                for rewiev, label in validloader:\n",
    "                    \n",
    "                    # Move review and label to device\n",
    "                    review, label = review.to(device), label.to(device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    output, hidden_v = model(review, hidden_v)\n",
    "                    \n",
    "                    # Compute validation loss\n",
    "                    loss_v += loss_function(output.squeeze(), label.float()).item()\n",
    "                    \n",
    "            print(f\"--- Epoch {epoch}/{epochs}\")\n",
    "            print(f\"    Step: {steps}\")\n",
    "            print(f\"    Validation Loss: {loss_v/len(validloader):.5f}\")\n",
    "                    \n",
    "            # Model back to train mode\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we are ready to test our model and use it for inference. We can start to put the  model in evaluation mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(74073, 500)\n",
       "  (lstm): LSTM(500, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using the model for inference, we want to look at his perfromance on the test set which is comoposed of previously unseen examples. We can esaily compute the accuracy in addition to the test loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.48547\n",
      "Accuracy: 86.06\n"
     ]
    }
   ],
   "source": [
    "# Keep track of test scores and labels\n",
    "testscores = -np.ones((len(testloader) * batch_size,))\n",
    "testlabels = -np.ones((len(testloader) * batch_size,))\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Reset hidden state\n",
    "    hidden_t = None\n",
    "    \n",
    "    # Training loss\n",
    "    loss_t = 0\n",
    "    \n",
    "    # Number of correct predictions\n",
    "    n_correct = 0\n",
    "    \n",
    "    for i, (review, label) in enumerate(testloader):\n",
    "        \n",
    "        assert review.shape == (batch_size, len_sequence)\n",
    "        \n",
    "        # Move review and label to device\n",
    "        review, label = review.to(device), label.to(device)\n",
    "                    \n",
    "        # Forward pass\n",
    "        output, hidden_v = model(review, hidden_v)\n",
    "                    \n",
    "        # Compute test loss\n",
    "        loss_t += loss_function(output.squeeze(), label.float()).item()\n",
    "        \n",
    "        # Compute test scores and store test labels\n",
    "        testscores[i*batch_size:(i+1)*batch_size] = output.squeeze().cpu()\n",
    "        testlabels[i*batch_size:(i+1)*batch_size] = label.squeeze().cpu()\n",
    "    \n",
    "        # Compute predictions by rounding scores to 0 or 1\n",
    "        predictions = torch.round(output.squeeze())\n",
    "        \n",
    "        # Compute number of correct predictions\n",
    "        correct = predictions == label.float().view_as(predictions)\n",
    "        correct = correct.squeeze().cpu().numpy()\n",
    "        n_correct += np.sum(correct)\n",
    "        \n",
    "print(f\"Test Loss: {loss_t / len(testloader):.5f}\")\n",
    "print(f\"Accuracy: {n_correct / (len(testloader) * batch_size) * 100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classifier we can easily compute the false positive rate and the true positive rate in order to plot the receiver operating characteristic (ROC) curve. The area under the ROC curve (AUC) is a measure of the classifier performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get false positive rate (FPR) and true positive rate (TPR)\n",
    "fpr, tpr, thresholds = roc_curve(testlabels, testscores)\n",
    "\n",
    "# Compute area under the ROC curve (AUC)\n",
    "AUC = roc_auc_score(testlabels, testscores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {AUC:.3f}\")\n",
    "plt.plot(np.linspace(0,1,100), np.linspace(0,1,100), \"k--\", label=\"random\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform inference with out trained model we need to transform a review into a pre-processed (and truncated/padded) sequence of encoded word. The can implement all the necessary pre-processing steps in an utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_tokens(review, len_sequence):\n",
    "    \"\"\"\n",
    "    Transform review (text) to padded sequence of word tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    from string import punctuation\n",
    "\n",
    "    # All owercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    review = \"\".join([char for char in review if char not in punctuation])\n",
    "    \n",
    "    # Split words\n",
    "    review = np.array(review.split())\n",
    "    \n",
    "    # Encode review word by word\n",
    "    encodedreview = [[word2int[word] for word in review]]\n",
    "    \n",
    "    # Truncate or pad review\n",
    "    paddedreview = np.zeros((1, len_sequence), dtype=int)\n",
    "    paddedreview[:,-len(review):] = encodedreview[:len_sequence]\n",
    "    \n",
    "    # Return review ready for model input\n",
    "    return paddedreview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the function behaves as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  65543 23548 71425 21200]]\n"
     ]
    }
   ],
   "source": [
    "paddedreview = review_to_tokens(\"The film was epic!\", len_sequence)\n",
    "\n",
    "assert paddedreview.shape == (1, len_sequence)\n",
    "\n",
    "print(paddedreview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicitly, we can also wrap inference in a function that only takes the text to analyze and a model as input and prints the corresponging score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(text, model, len_sequence):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = review_to_tokens(text, len_sequence)\n",
    "    \n",
    "    tokens = torch.from_numpy(tokens).to(device)\n",
    "    \n",
    "    hidden = None\n",
    "    \n",
    "    output, hidden = model(tokens, hidden)\n",
    "    \n",
    "    score = output.squeeze().item()\n",
    "    \n",
    "    prediction = np.round(score)\n",
    "    \n",
    "    print(f\"Score: {score:.2f} | Prediction: {prediction} | {'Positive' if prediction else 'Negative'} Review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally use the model on some new review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.85 | Prediction: 1.0 | Positive Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"The movie was great!\", model, len_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.81 | Prediction: 1.0 | Positive Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"The movie had some good and bad parts, but I would suggest to watch it!\", model, len_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.05 | Prediction: 0.0 | Negative Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"The movie had some good and bad parts, and I would suggest not to watch it!\", model, len_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.53 | Prediction: 1.0 | Positive Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"I'm not sure how to feel about the movie.\", model, len_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.13 | Prediction: 0.0 | Negative Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"You should never, ever go to watch this movie. It's terrible!\", model, len_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.99 | Prediction: 1.0 | Positive Review\n"
     ]
    }
   ],
   "source": [
    "analysis(\"You should never miss this movie!\", model, len_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
