{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fae7836b590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs things first, we have to load the training data. In order to train our character-level LSTM network we use the book _Beyond Good and Evil_ by Friedrich Nietzsche as released by the [Gutemberg PRoject](https://www.gutenberg.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"data/Beyond-Good-and-Evil.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER I. PREJUDICES OF PHILOSOPHERS\n",
      "\n",
      "\n",
      "1. The Will to Truth, which is to tempt us to many a hazardous\n",
      "enterprise, the famous Truthfulness of which all philosophers have\n",
      "hitherto spoken with respect, what questions has this Will to Truth not\n",
      "laid before us! What strange, perplexing, questionable questions! It is\n",
      "already a long story; yet it seems as if it were hardly commenced. Is\n",
      "it any wonder if we at last grow distrustful, lose patience, and turn\n",
      "impatiently away? That this Sphinx teaches us \n"
     ]
    }
   ],
   "source": [
    "# Load file in one string\n",
    "with open(fname, \"r\") as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Remove header\n",
    "text = text[328:]\n",
    "    \n",
    "# Print firs 500 character of text\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 373094\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text is composed of character and a character-level LSTM network will produce new text character by character. We can extract all the character of the text to see the number of tokens our model will work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 78\n"
     ]
    }
   ],
   "source": [
    "tokens = sorted(set(text))\n",
    "n_tokens =  len(tokens)\n",
    "print(f\"Unique characters: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to map the characters (tokens) to a unique integer that can be understood by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, 'A': 23, 'B': 24, 'C': 25, 'D': 26, 'E': 27, 'F': 28, 'G': 29, 'H': 30, 'I': 31, 'J': 32, 'K': 33, 'L': 34, 'M': 35, 'N': 36, 'O': 37, 'P': 38, 'Q': 39, 'R': 40, 'S': 41, 'T': 42, 'U': 43, 'V': 44, 'W': 45, 'X': 46, 'Y': 47, 'Z': 48, '[': 49, ']': 50, '_': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77}\n"
     ]
    }
   ],
   "source": [
    "char2int = {c : i for i, c in enumerate(tokens)}\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to revert the integer encoding to the original characters we also need to define the inverse mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '?', 23: 'A', 24: 'B', 25: 'C', 26: 'D', 27: 'E', 28: 'F', 29: 'G', 30: 'H', 31: 'I', 32: 'J', 33: 'K', 34: 'L', 35: 'M', 36: 'N', 37: 'O', 38: 'P', 39: 'Q', 40: 'R', 41: 'S', 42: 'T', 43: 'U', 44: 'V', 45: 'W', 46: 'X', 47: 'Y', 48: 'Z', 49: '[', 50: ']', 51: '_', 52: 'a', 53: 'b', 54: 'c', 55: 'd', 56: 'e', 57: 'f', 58: 'g', 59: 'h', 60: 'i', 61: 'j', 62: 'k', 63: 'l', 64: 'm', 65: 'n', 66: 'o', 67: 'p', 68: 'q', 69: 'r', 70: 's', 71: 't', 72: 'u', 73: 'v', 74: 'w', 75: 'x', 76: 'y', 77: 'z'}\n"
     ]
    }
   ],
   "source": [
    "int2char = {i: c for c, i in char2int.items()}\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the conversion between char and ints\n",
    "for t in tokens:\n",
    "    assert t == int2char[char2int[t]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `char2int` msapping we can finally encode the whole text (list of characters) into a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 30 23 38 42 27 40  1 31  9  1 38 40 27 32 43 26 31 25 27 41  1 37 28\n",
      "  1 38 30 31 34 37 41 37 38 30 27 40 41  0  0  0 11  9  1 42 59 56  1 45\n",
      " 60 63 63  1 71 66  1 42 69 72 71 59  7  1 74 59 60 54 59  1 60 70  1 71\n",
      " 66  1 71 56 64 67 71  1 72 70  1 71 66  1 64 52 65 76  1 52  1 59 52 77\n",
      " 52 69 55 66 72 70  0 56 65 71 56 69 67 69 60 70 56  7  1 71 59 56  1 57\n",
      " 52 64 66 72 70  1 42 69 72 71 59 57 72 63 65 56 70 70  1 66 57  1 74 59\n",
      " 60 54 59  1 52 63 63  1 67 59 60 63 66 70 66 67 59 56 69 70  1 59 52 73\n",
      " 56  0 59 60 71 59 56 69 71 66  1 70 67 66 62 56 65  1 74 60 71 59  1 69\n",
      " 56 70 67 56 54 71  7  1 74 59 52 71  1 68 72 56 70 71 60 66 65 70  1 59\n",
      " 52 70  1 71 59 60 70  1 45 60 63 63  1 71 66  1 42 69 72 71 59  1 65 66\n",
      " 71  0 63 52 60 55  1 53 56 57]\n"
     ]
    }
   ],
   "source": [
    "# Encode text mapping characters to integers\n",
    "encodedtext = np.array([char2int[char] for char in text])\n",
    "\n",
    "print(encodedtext[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model will take input in one-hot encoded form and therefore we need to write a function to transform our integer-encoded tokens into one-hot encoded tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(data, num_labels):\n",
    "    \"\"\"\n",
    "    One hot encoding of integer-encoded data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform data to numpy array\n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Initialize one-hot encoding vector\n",
    "    # PyTorch standard type is torch.float32\n",
    "    # Declare numpy array as np.float32 to avoid conversion errors\n",
    "    one_hot = np.zeros((data.size, num_labels), dtype=np.float32)\n",
    "    \n",
    "    # Row indices for hot elements (all rows)\n",
    "    row_idx = np.arange(one_hot.shape[0])\n",
    "    \n",
    "    # Data contains integer-encoded characters\n",
    "    # Hot element column indices correspond to their value\n",
    "    col_idx = data.flatten()\n",
    "    \n",
    "    # Perform one-hot encoding\n",
    "    one_hot[row_idx,col_idx] = 1.0\n",
    "    \n",
    "    # Reshape one-hot encoding with original data shape\n",
    "    # An additional dimension for the one-hot encoding is added\n",
    "    one_hot = one_hot.reshape((*data.shape, num_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally test that `one_hot_encoder` performs as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25 30 23 38 42]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Number of elements to one-hot encode in this test\n",
    "n = 5\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_test = one_hot_encoder(encodedtext[:n], n_tokens)\n",
    "\n",
    "assert one_hot_test.shape == (n, n_tokens)\n",
    "\n",
    "for idx, e in enumerate(encodedtext[:n]):\n",
    "    assert one_hot_test[idx, e] == 1\n",
    "    \n",
    "print(encodedtext[:n])\n",
    "print(one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we want to create mini-barches for training. With text, we want to split the text in different sequences (of length `len_sequence`) and pool them in multiple batches that will be fed to our model simultaneously. This means that each batch will have a total size of `num_char_batch = batch_size * len_sequence` characters: `batch_size` indicates the number of sequences in a batch, not the number of characters (tokens). In order to make things easier we can discard the last partial batch and retain a total number of  batches given by `num_chars // num_char_batch`, where `num_chars` is the total number of characters in the training text used for training. Once the the data has been trimmed, we can split it in `batch_size` batches, by re-shaping the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in a sequence: 3\n",
      "Number of sequences in a batch: 4\n",
      "Number of batches: 3\n",
      "Raw data:\n",
      "[ 1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6  7  7  7  8  8  8\n",
      "  9  9  9 10 10 10 11 11 11 12 12 12 13 13 13 14 14 14]\n",
      "Trimmed data:\n",
      "[ 1  1  1  2  2  2  3  3  3  4  4  4  5  5  5  6  6  6  7  7  7  8  8  8\n",
      "  9  9  9 10 10 10 11 11 11 12 12 12]\n",
      "Reshaped data:\n",
      "[[ 1  1  1  2  2  2  3  3  3]\n",
      " [ 4  4  4  5  5  5  6  6  6]\n",
      " [ 7  7  7  8  8  8  9  9  9]\n",
      " [10 10 10 11 11 11 12 12 12]]\n"
     ]
    }
   ],
   "source": [
    "len_sequence_test = 3 # Test sequence length\n",
    "batch_size_test = 4 # Test batch size (number of sequences per batch)\n",
    "\n",
    "print(f\"Number of characters in a sequence: {len_sequence_test}\")\n",
    "print(f\"Number of sequences in a batch: {batch_size_test}\")\n",
    "\n",
    "num_characters = 42 # Total number of characters \n",
    "\n",
    "# Number of characters per batch\n",
    "num_char_batch_test = len_sequence_test * batch_size_test\n",
    "\n",
    "# Actual number of batches\n",
    "# Total number of characters divided by the number of characters per batch\n",
    "# Integer division is performed with //\n",
    "num_batches_test = num_characters // num_char_batch_test\n",
    "\n",
    "print(f\"Number of batches: {num_batches_test}\")\n",
    "\n",
    "# Create fictitious data with different numbers for characters of different sequences\n",
    "data, seqence_idx = [], 0\n",
    "for idx in range(num_characters):\n",
    "    if idx % len_sequence_test == 0:\n",
    "        seqence_idx += 1\n",
    "        \n",
    "    data.append(seqence_idx)\n",
    "        \n",
    "data = np.array(data)\n",
    "print(f\"Raw data:\\n{data}\")\n",
    "\n",
    "# Trim data to have only complete batches\n",
    "data = data[:num_batches_test * num_char_batch_test]\n",
    "print(f\"Trimmed data:\\n{data}\")\n",
    "\n",
    "data = data.reshape(batch_size_test, -1)\n",
    "print(f\"Reshaped data:\\n{data}\")\n",
    "\n",
    "assert data.shape == (batch_size_test, num_batches_test * len_sequence_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After re-shaping we have the data in `batch_size` rows, which is the number of sequences we want to use in one batch (and not the number of batches). We see that the `num_batches` are clearly separated along columns and have length `len_sequence`  along rows. Therefore, we can slide over the columns a window of length `len_sqeuence` to create `num_batches` batches (of size `len_sequence * batch_size`) for the input and target variables. The inputs are simply the window of shape `(batch_size, len_seqence)` sliding over the data in steps of `len_seqence`; the target is the same window shifted by one. Care should be taken for the last index, that needs to wrap around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched data:\n",
      "[[0 0 0 1 1 1 2 2 2]\n",
      " [0 0 0 1 1 1 2 2 2]\n",
      " [0 0 0 1 1 1 2 2 2]\n",
      " [0 0 0 1 1 1 2 2 2]]\n"
     ]
    }
   ],
   "source": [
    "# Show structure of different batches\n",
    "for idx, n in enumerate(range(0, num_batches_test * len_sequence_test, len_sequence_test)):\n",
    "    data[:,n:n + len_sequence_test] = idx\n",
    "\n",
    "print(f\"Batched data:\\n{data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode this complicated batching process in a function that yield the current batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterbatches(data, batch_size, len_sequence):\n",
    "    \n",
    "    # Number of characters per batch\n",
    "    num_char_batch = batch_size * len_sequence\n",
    "    \n",
    "    # Total number of characters\n",
    "    num_chars = len(data)\n",
    "    \n",
    "    # Total number of full batches\n",
    "    # // performs integer division\n",
    "    num_batches = num_chars // num_char_batch\n",
    "    \n",
    "    # Discard last charachters not filling a batch\n",
    "    data = data[:num_batches * num_char_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    data = data.reshape((batch_size, -1))\n",
    "    \n",
    "    assert data.shape[1] == num_batches * len_sequence\n",
    "    \n",
    "    for n in range(0, num_batches * len_sequence, len_sequence):\n",
    "        \n",
    "        # Input features\n",
    "        inputs = data[:,n:n + len_sequence]\n",
    "        \n",
    "        # Target features\n",
    "        # Input features shifted by one\n",
    "        targets = np.zeros_like(inputs)\n",
    "        targets[:,:-1] = inputs[:,1:] # Shift input by one\n",
    "        try:\n",
    "            targets[:,-1] = data[:,n + len_sequence] # Add last element\n",
    "        except IndexError: # Last batch, wrap around \n",
    "            targets[:,-1] = data[:,0]\n",
    "        \n",
    "        # Yeld \n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test the `iterbarches` function to make sure that its output is what we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence (0):\n",
      "[[25 30 23 38 42 27 40  1 31  9]\n",
      " [70 71 60 54  1 66 57  1 71 59]\n",
      " [ 1 66 57  1 71 59 56  1 71 69]]\n",
      "Target Sequence (0):\n",
      "[[30 23 38 42 27 40  1 31  9  1]\n",
      " [71 60 54  1 66 57  1 71 59 56]\n",
      " [66 57  1 71 59 56  1 71 69 72]]\n",
      "Input Sequence (1):\n",
      "[[ 1 38 40 27 32 43 26 31 25 27]\n",
      " [56  1 71 76 67 56  1  3 57 69]\n",
      " [72 71 59 57 72 63  8  8 71 59]]\n",
      "Target Sequence (2):\n",
      "[[38 40 27 32 43 26 31 25 27 41]\n",
      " [ 1 71 76 67 56  1  3 57 69 56]\n",
      " [71 59 57 72 63  8  8 71 59 56]]\n"
     ]
    }
   ],
   "source": [
    "testbatches = iterbatches(encodedtext, batch_size=3, len_sequence=10)\n",
    "\n",
    "inputs, targets = next(testbatches)\n",
    "\n",
    "assert inputs.shape == (3, 10)\n",
    "assert targets.shape == (3, 10)\n",
    "\n",
    "print(f\"Input Sequence (0):\\n{inputs}\")\n",
    "print(f\"Target Sequence (0):\\n{targets}\")\n",
    "\n",
    "inputs, targets = next(testbatches)\n",
    "\n",
    "assert inputs.shape == (3, 10)\n",
    "assert targets.shape == (3, 10)\n",
    "\n",
    "print(f\"Input Sequence (1):\\n{inputs}\")\n",
    "print(f\"Target Sequence (2):\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the target sequence is the input sequence shifted by one on the `len_sequence` dimension (`axis=1`). It's also obvious that the last element of the last batch of the target sequence is taken from the next set of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define out character-level LSTM network, composed of a `nn.LSTM` module (with `n_layers` LSTM layers and a hidden state output of size `n_hidden`) and a fully connected layer taking `n_tokens` input features (corresponding to the one-hot encoding of the integer-encoded tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_tokens, n_hidden=256, n_layers=2, pdrop=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Number of features in  the LSTM hidden state\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        # Number of LSTM hidden layers\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Dropout probability\n",
    "        self.pdrop = pdrop\n",
    "        \n",
    "        # Define \n",
    "        self.lstm = nn.LSTM(\n",
    "            n_tokens, \n",
    "            n_hidden, \n",
    "            n_layers, \n",
    "            dropout=pdrop, # LSTM dropout\n",
    "            batch_first=True # Batch dimension is first\n",
    "        )\n",
    "        \n",
    "        # Dropout layer for input of the  fully connected layer\n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, n_tokens)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Forward pass in LSTM\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Stack LSTM outputs\n",
    "        # First dimension is batches\n",
    "        output = output.view(-1, self.n_hidden)\n",
    "        \n",
    "        # Forward pass through fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # Return log probabilities\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        \n",
    "        # Return output and hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make sure that the architecture works correctly, we can test a single forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate CharLSTM with a hidden output size of 64\n",
    "n_layers = 2\n",
    "testlstm = CharLSTM(n_tokens, n_hidden=64, n_layers=n_layers)\n",
    "\n",
    "# Get one batch of integer-encoded data\n",
    "testbatches = iterbatches(encodedtext, batch_size=3, len_sequence=10)\n",
    "inputs, targets = next(testbatches)\n",
    "\n",
    "# Perform one-hot encoding and transform to PyTorch tensor\n",
    "inputs = one_hot_encoder(inputs, n_tokens)\n",
    "inputs = torch.from_numpy(inputs)\n",
    "\n",
    "# Forward pass\n",
    "output, hidden = testlstm(inputs, None)\n",
    "\n",
    "# Check output shape: (batch_size * len_sequence, n_tokens)\n",
    "assert output.shape == (3 * 10, n_tokens) \n",
    "\n",
    "# Test hidden is a tuple with n_layers elements\n",
    "assert len(hidden) == n_layers \n",
    "\n",
    "for h in hidden:\n",
    "    # Test hidden output shape for each layer (n_layers, batch_size, n_hidden)\n",
    "    assert h.shape == (n_layers, 3, 64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally define out training loop as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          optimizer, \n",
    "          loss_function, \n",
    "          data,\n",
    "          n_tokens,\n",
    "          epochs=10, \n",
    "          batch_size=10,\n",
    "          len_sequence=50, \n",
    "          clip=5, \n",
    "          print_every=5,\n",
    "          device=device):\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Move model to devide\n",
    "    model.to(device)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = None\n",
    "    \n",
    "        for inputs, targets in iterbatches(data, batch_size, len_sequence):\n",
    "            \n",
    "            assert inputs.shape == targets.shape == (batch_size, len_sequence)\n",
    "            \n",
    "            inputs = one_hot_encoder(inputs, n_tokens)\n",
    "            \n",
    "            # Initialise tensors and move to device\n",
    "            inputs = torch.from_numpy(inputs).to(device)\n",
    "            targets = torch.from_numpy(targets).to(device)\n",
    "            \n",
    "            output, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Detach all hidden states from the computational graph\n",
    "            # Avoid backpropagation through the entire history\n",
    "            # Hidden states are stored in a tuple of size n_layers\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            \n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_function(output, targets.view(batch_size * len_sequence))\n",
    "            \n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # Accumulate epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Clip gradients norm\n",
    "            # Prevents the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            # Optimise model parameters\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            num_batches = data.size // (batch_size * len_sequence)\n",
    "            \n",
    "            print(f\"--- Epoch {epoch:2}/{epochs:2} ---\")\n",
    "            print(f\"Loss: {epoch_loss/num_batches:.5f}\")\n",
    "            print(f\"Time: {stop_time-start_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (lstm): LSTM(78, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=78, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = CharLSTM(n_tokens, n_hidden, n_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch  0/20 ---\n",
      "Loss: 3.39770\n",
      "Time: 56.45 s\n",
      "--- Epoch  1/20 ---\n",
      "Loss: 3.17779\n",
      "Time: 57.29 s\n",
      "--- Epoch  2/20 ---\n",
      "Loss: 3.16591\n",
      "Time: 58.42 s\n",
      "--- Epoch  3/20 ---\n",
      "Loss: 3.10668\n",
      "Time: 59.34 s\n",
      "--- Epoch  4/20 ---\n",
      "Loss: 2.93153\n",
      "Time: 52.65 s\n",
      "--- Epoch  5/20 ---\n",
      "Loss: 2.66057\n",
      "Time: 51.18 s\n",
      "--- Epoch  6/20 ---\n",
      "Loss: 2.52134\n",
      "Time: 51.73 s\n",
      "--- Epoch  7/20 ---\n",
      "Loss: 2.42938\n",
      "Time: 51.04 s\n",
      "--- Epoch  8/20 ---\n",
      "Loss: 2.34566\n",
      "Time: 51.55 s\n",
      "--- Epoch  9/20 ---\n",
      "Loss: 2.27707\n",
      "Time: 51.82 s\n",
      "--- Epoch 10/20 ---\n",
      "Loss: 2.21662\n",
      "Time: 52.12 s\n",
      "--- Epoch 11/20 ---\n",
      "Loss: 2.16436\n",
      "Time: 52.18 s\n",
      "--- Epoch 12/20 ---\n",
      "Loss: 2.11680\n",
      "Time: 52.27 s\n",
      "--- Epoch 13/20 ---\n",
      "Loss: 2.07256\n",
      "Time: 52.02 s\n",
      "--- Epoch 14/20 ---\n",
      "Loss: 2.03249\n",
      "Time: 51.68 s\n",
      "--- Epoch 15/20 ---\n",
      "Loss: 1.99457\n",
      "Time: 52.59 s\n",
      "--- Epoch 16/20 ---\n",
      "Loss: 1.95888\n",
      "Time: 53.07 s\n",
      "--- Epoch 17/20 ---\n",
      "Loss: 1.92814\n",
      "Time: 52.38 s\n",
      "--- Epoch 18/20 ---\n",
      "Loss: 1.89683\n",
      "Time: 52.68 s\n",
      "--- Epoch 19/20 ---\n",
      "Loss: 1.87209\n",
      "Time: 52.13 s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "len_sequence = 100\n",
    "epochs = 20\n",
    "\n",
    "#  Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Loss function\n",
    "# NLLLoss + LogSoftmax output is equivalent to CrossEntropyLoss\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "#  Train model\n",
    "train(model, optimizer, loss_function, encodedtext, n_tokens, \n",
    "      epochs=epochs, batch_size=batch_size, len_sequence=len_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training we can save the model for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make directory for models\n",
    "try:\n",
    "    os.mkdir(\"models\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "checkpoint = {\n",
    "    \"n_hidden\": model.n_hidden,\n",
    "    \"n_layers\": model.n_layers,\n",
    "    \"state_dict\": model.state_dict(),\n",
    "}\n",
    "        \n",
    "torch.save(checkpoint, \"models/CharLSTM.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A character-level LSTM network gives a probability distribution for the next character in a sequence (among all possible characters), given the previous character and an hidden state (memory of the network). We can therefore define an helper function that given a character and an hidden state predicts the next character from the probability distribution over all possible character. Such probability distribution is obtained by applying a `F.softmax` function to the raw output ofthe newtowrk. Instead of using the most probable character only, we can use a `top_k` policy, where the next characer is randomly selected among the top $k$ most probable ones (with a probability proportional to their original probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(char, hidden, model, tokens, top_k=3):\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Transform char to integer encoding\n",
    "    inputs = np.array([[char2int[char]]])\n",
    "    \n",
    "    # One-hot encode input\n",
    "    n_tokens = len(tokens)\n",
    "    inputs = one_hot_encoder(inputs, n_tokens)\n",
    "    \n",
    "    # Transform numpy array to torch tensor\n",
    "    inputs = torch.from_numpy(inputs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Propagation  through the network\n",
    "        output, hidden = model(inputs, hidden)\n",
    "\n",
    "        # Get probability distribution for next character\n",
    "        # Fist dimension is batches\n",
    "        # Network output is LogSoftmax\n",
    "        probabilities = torch.exp(output)\n",
    "        \n",
    "        # Get top characters\n",
    "        p, top_char = probabilities.topk(top_k)\n",
    "        top_char = top_char.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Select next character amont top_k most probable\n",
    "        # Assign probabilities proportional to predicted probability\n",
    "        p = p.cpu().numpy().squeeze()\n",
    "        nextchar = np.random.choice(top_char, p=p/p.sum())\n",
    "        \n",
    "        # Return predicted char and hidden state\n",
    "        return int2char[nextchar], hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can generate new text. We start with a prime input that is used to initialise the hidden state of the network, then new characters are sampled using the LSTM network prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, length, prime, top_k=3):\n",
    "    \n",
    "    # List of prime characters\n",
    "    chars = list(prime)\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Run on prime\n",
    "    for char in chars:\n",
    "        char, hidden = predict(char, hidden, model, tokens, top_k=top_k)\n",
    "\n",
    "    # Append first prediction after prime\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Use previous prediction to obtain a new prediction\n",
    "    for _ in range(length):\n",
    "        char, hidden = predict(chars[-1], hidden, model, tokens, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a matter of fact, the manterents\n",
      "the more, the stint and the serest, and allose the ponered and the sore the stiling, to the profount to bat the moral at allost an and and an the store, the\n",
      "constrection of the most along there\n",
      "as allow an is andest a precies the\n",
      "postined, the perhaps and to the more also of things and so a prosound of the most of an a could this\n",
      "the soul of the possed,\" and\n",
      "alse stan that and some a some of the spiest and a moral of the stile to beloode a sente ther are and self-to beligetion tastery, and stours, and the sour of self a soul to the meant of\n",
      "the sente of the senter and to that an a sould to the more the posers and the still to that in to sees, the some and senter, the man only in alout that the prosounes to be all allowing of the precient of the\n",
      "strongion, and along, to the some as the more of the presance, which the prose of the man a something\n",
      "of that the still and allatity, and and senses of the sporiting and the self an incentions of a simplices the postents to alloothers\n",
      "only the some that as to so enere to\n",
      "be the sented to be to\n",
      "be of\n",
      "the sense to\n",
      "sention of the spoes and the somation and alsase that a manterand a somethance things and toured the possality and to to be sume the mensant, and sense and to the manting this to the sente and the proficition and and to a sange a manter to a sore\n",
      "and streaged and\n",
      "selficing to to be to the something and a soul of the serperes of the store of and a modert to bite one somethan the soul to the most the mase all that\n",
      "it is the soul to the modern alood the preasion and a sour and a sentination of the stranghing a self--who and the\n",
      "sencation and and this the prilines to be the porens and sense this the soul of suce and and\n",
      "all the\n",
      "sente the self therefore in the selinally as itserfiness and there are the presaled the many to and that it the mank to a postious of a possing as a contertand and an a centining as the most all the pricess, the soul ore that a conserted to the proses of the mentined and the profoned\n"
     ]
    }
   ],
   "source": [
    "generatedtext = sample(model, 2000, \"As a matter of fact,\")\n",
    "print(generatedtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text is far from perfect, as expected from a character-level LSTM network. However it could be improved by playing with the hyperparameters of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
