{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-Level LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1711d63f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files = \"data/DivinaCommedia/Inferno\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Nel mezzo del cammin di nostra vita\n",
      "mi ritrovai per una selva oscura,\n",
      "ché la diritta via era smarrita.\n",
      "\n",
      "Ahi quanto a dir qual era è cosa dura\n",
      "esta selva selvaggia e aspra e forte\n",
      "che nel pensier rinova la paura!\n",
      "\n",
      "Tant' è amara che poco è più morte;\n",
      "ma per trattar del ben ch'i' vi trovai,\n",
      "dirò de l'altre cose ch'i' v'ho scorte.\n",
      "\n",
      "Io non so ben ridir com' i' v'intrai,\n",
      "tant' era pien di sonno a quel punto\n",
      "che la verace via abbandonai.\n",
      "\n",
      "Ma poi ch'i' fui al piè d'un colle giunto,\n",
      "là dove terminava qu\n"
     ]
    }
   ],
   "source": [
    "# List files (sorted)\n",
    "files = sorted(os.listdir(path_to_files))\n",
    "\n",
    "# Load all files in one string\n",
    "text = \"\"\n",
    "for file in files:\n",
    "    fname = os.path.join(path_to_files, file)\n",
    "    with open(fname, \"r\") as f:\n",
    "        text += f.read()\n",
    "        \n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 172243\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of characters: {len(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 69\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(set(text))\n",
    "print(f\"Unique characters: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: ',', 6: '-', 7: '.', 8: ':', 9: ';', 10: '?', 11: 'A', 12: 'B', 13: 'C', 14: 'D', 15: 'E', 16: 'F', 17: 'G', 18: 'H', 19: 'I', 20: 'L', 21: 'M', 22: 'N', 23: 'O', 24: 'P', 25: 'Q', 26: 'R', 27: 'S', 28: 'T', 29: 'U', 30: 'V', 31: 'Z', 32: '`', 33: 'a', 34: 'b', 35: 'c', 36: 'd', 37: 'e', 38: 'f', 39: 'g', 40: 'h', 41: 'i', 42: 'l', 43: 'm', 44: 'n', 45: 'o', 46: 'p', 47: 'q', 48: 'r', 49: 's', 50: 't', 51: 'u', 52: 'v', 53: 'x', 54: 'z', 55: '«', 56: '»', 57: 'à', 58: 'ä', 59: 'è', 60: 'é', 61: 'ë', 62: 'ì', 63: 'ï', 64: 'ò', 65: 'ó', 66: 'ö', 67: 'ù', 68: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "int2char = dict(enumerate(chars))\n",
    "print(int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, ',': 5, '-': 6, '.': 7, ':': 8, ';': 9, '?': 10, 'A': 11, 'B': 12, 'C': 13, 'D': 14, 'E': 15, 'F': 16, 'G': 17, 'H': 18, 'I': 19, 'L': 20, 'M': 21, 'N': 22, 'O': 23, 'P': 24, 'Q': 25, 'R': 26, 'S': 27, 'T': 28, 'U': 29, 'V': 30, 'Z': 31, '`': 32, 'a': 33, 'b': 34, 'c': 35, 'd': 36, 'e': 37, 'f': 38, 'g': 39, 'h': 40, 'i': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'q': 47, 'r': 48, 's': 49, 't': 50, 'u': 51, 'v': 52, 'x': 53, 'z': 54, '«': 55, '»': 56, 'à': 57, 'ä': 58, 'è': 59, 'é': 60, 'ë': 61, 'ì': 62, 'ï': 63, 'ò': 64, 'ó': 65, 'ö': 66, 'ù': 67, 'ü': 68}\n"
     ]
    }
   ],
   "source": [
    "char2int = {char: i for i, char in int2char.items()}\n",
    "print(char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 22 37 42  1 43 37 54 54 45  1 36 37 42  1 35 33 43 43 41 44  1 36 41\n",
      "  1 44 45 49 50 48 33  1 52 41 50 33  0 43 41  1 48 41 50 48 45 52 33 41\n",
      "  1 46 37 48  1 51 44 33  1 49 37 42 52 33  1 45 49 35 51 48 33  5  0 35\n",
      " 40 60  1 42 33  1 36 41 48 41 50 50 33  1 52 41 33  1 37 48 33  1 49 43\n",
      " 33 48 48 41 50 33  7  0  0 11 40 41  1 47 51 33 44 50 45  1 33  1 36 41\n",
      " 48  1 47 51 33 42  1 37 48 33  1 59  1 35 45 49 33  1 36 51 48 33  0 37\n",
      " 49 50 33  1 49 37 42 52 33  1 49 37 42 52 33 39 39 41 33  1 37  1 33 49\n",
      " 46 48 33  1 37  1 38 45 48 50 37  0 35 40 37  1 44 37 42  1 46 37 44 49\n",
      " 41 37 48  1 48 41 44 45 52 33  1 42 33  1 46 33 51 48 33  2  0  0 28 33\n",
      " 44 50  4  1 59  1 33 43 33 48 33  1 35 40 37  1 46 45 35 45  1 59  1 46\n",
      " 41 67  1 43 45 48 50 37  9  0]\n"
     ]
    }
   ],
   "source": [
    "# Encode text mapping characters to integers\n",
    "encoded = np.array([char2int[char] for char in text])\n",
    "\n",
    "print(encoded[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(data, num_labels):\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    \n",
    "    # Initialize one-hot encoding vector\n",
    "    # PyTorch standard type is torch.float32\n",
    "    # Declare numpy array as np.float32 to avoid conversion errors\n",
    "    one_hot = np.zeros((data.size, num_labels), dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), data.flatten()] = 1.0\n",
    "    \n",
    "    one_hot = one_hot.reshape((*data.shape, num_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_test = one_hot_encoder(encoded[:5], len(chars))\n",
    "\n",
    "assert one_hot_test.shape == (5, len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 22 37 42  1]\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:5])\n",
    "print(one_hot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterbatches(data, batch_size, len_sequence):\n",
    "    \n",
    "    # Number of characters per batch\n",
    "    num_char_batch = batch_size * len_sequence\n",
    "    \n",
    "    # Total number of characters\n",
    "    num_chars = len(data)\n",
    "    \n",
    "    # Total number of full batches\n",
    "    num_batches = num_chars // num_char_batch\n",
    "    \n",
    "    # Discard last charachters not filling a batch\n",
    "    data = data[:num_batches * num_char_batch]\n",
    "    \n",
    "    # Reshape into batch_size rows\n",
    "    data = data.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, data.shape[1], len_sequence):\n",
    "        \n",
    "        # Input features\n",
    "        inputs = data[:,n:n + len_sequence]\n",
    "        \n",
    "        # Target features\n",
    "        # Input features shifted by one\n",
    "        targets = np.zeros_like(inputs)\n",
    "        targets[:,:-1] = inputs[:,1:] # Shift input by one\n",
    "        try:\n",
    "            targets[:,-1] = data[:,n + len_sequence] # Add last element\n",
    "        except IndexError:\n",
    "            targets[:,-1] = data[:,0]\n",
    "        \n",
    "        # Yeld \n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence Shape:\n",
      "(3, 10)\n",
      "Target Sequence Shape:\n",
      "(3, 10)\n",
      "Input Sequence:\n",
      "[[ 1 22 37 42  1 43 37 54 54 45]\n",
      " [ 0 49 33 37 50 50 33 44 36 45]\n",
      " [49 49 37  5  1 33 44 35 45 48]]\n",
      "Target Sequence:\n",
      "[[22 37 42  1 43 37 54 54 45  1]\n",
      " [49 33 37 50 50 33 44 36 45  1]\n",
      " [49 37  5  1 33 44 35 45 48  1]]\n"
     ]
    }
   ],
   "source": [
    "testbatches = iterbatches(encoded, batch_size=3, len_sequence=10)\n",
    "\n",
    "inputs, targets = next(testbatches)\n",
    "\n",
    "assert inputs.shape == (3, 10)\n",
    "assert targets.shape == (3, 10)\n",
    "\n",
    "print(f\"Input Sequence Shape:\\n{inputs.shape}\")\n",
    "print(f\"Target Sequence Shape:\\n{targets.shape}\")\n",
    "\n",
    "print(f\"Input Sequence:\\n{inputs}\")\n",
    "print(f\"Target Sequence:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the target sequence is the input sequence shifted by one on the `len_sequence` dimension (`axis=1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, pdrop=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.pdrop = pdrop\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            len(tokens), \n",
    "            n_hidden, \n",
    "            n_layers, \n",
    "            dropout=pdrop, \n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(pdrop)\n",
    "        \n",
    "        self.fc = nn.Linear(n_hidden, len(tokens))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Forward pass in LSTM\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        # Stack LSTM outputs\n",
    "        output = output.view(-1, self.n_hidden)\n",
    "        \n",
    "        # Forward pass through fully connected layer\n",
    "        output = self.fc(output)\n",
    "        \n",
    "        # Return output and hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlstm = CharLSTM(chars, n_hidden=64)\n",
    "\n",
    "testbatches = iterbatches(encoded, batch_size=3, len_sequence=10)\n",
    "inputs, targets = next(testbatches)\n",
    "\n",
    "inputs = one_hot_encoder(inputs, len(chars))\n",
    "inputs = torch.from_numpy(inputs)\n",
    "\n",
    "output, hidden = testlstm(inputs, None)\n",
    "\n",
    "assert output.shape == (3 * 10, len(chars)) # (batch_size * len_sequence, len(chars))\n",
    "\n",
    "assert len(hidden) == 2 # hidden is a tuple with n_layers elements\n",
    "\n",
    "for h in hidden:\n",
    "    assert h.shape == (2, 3, 64) # (n_layers, batch_size, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          optimizer, \n",
    "          loss_function, \n",
    "          data,\n",
    "          epochs=10, \n",
    "          batch_size=10,\n",
    "          len_sequence=50, \n",
    "          clip=5, \n",
    "          print_every=5,\n",
    "          device=device):\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    # Set model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Move model to devide\n",
    "    model.to(device)\n",
    "    \n",
    "    num_tokens = len(set(data))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = None\n",
    "    \n",
    "        for inputs, targets in iterbatches(data, batch_size, len_sequence):\n",
    "            \n",
    "            assert inputs.shape == targets.shape == (batch_size, len_sequence)\n",
    "            \n",
    "            inputs = one_hot_encoder(inputs, num_tokens)\n",
    "            \n",
    "            # Initialise tensors and move to device\n",
    "            inputs = torch.from_numpy(inputs).to(device)\n",
    "            targets = torch.from_numpy(targets).to(device)\n",
    "            \n",
    "            output, hidden = model(inputs, hidden)\n",
    "            \n",
    "            # Detach all hidden states from the computational graph\n",
    "            # Avoid backpropagation through the entire history\n",
    "            # Hidden states are stored in a tuple of size n_layers\n",
    "            hidden = tuple(h.detach() for h in hidden)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = loss_function(output, targets.view(batch_size*len_sequence))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Accumulate epoch loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Clip gradients norm\n",
    "            # Prevents the exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            \n",
    "            # Optimise model parameters\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            num_batches = data.size // (batch_size * len_sequence)\n",
    "            \n",
    "            print(f\"--- Epoch {epoch:2}/{epochs:2} ---\")\n",
    "            print(f\"Loss: {epoch_loss/num_batches:.5f}\")\n",
    "            print(f\"Time: {stop_time-start_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (lstm): LSTM(69, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=69, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "\n",
    "model = CharLSTM(chars, n_hidden, n_layers)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch  0/25 ---\n",
      "Loss: 3.53031\n",
      "Time: 23.25 s\n",
      "--- Epoch  1/25 ---\n",
      "Loss: 3.14066\n",
      "Time: 25.73 s\n",
      "--- Epoch  2/25 ---\n",
      "Loss: 3.11649\n",
      "Time: 24.41 s\n",
      "--- Epoch  3/25 ---\n",
      "Loss: 3.10742\n",
      "Time: 24.18 s\n",
      "--- Epoch  4/25 ---\n",
      "Loss: 3.09931\n",
      "Time: 24.84 s\n",
      "--- Epoch  5/25 ---\n",
      "Loss: 3.08751\n",
      "Time: 24.56 s\n",
      "--- Epoch  6/25 ---\n",
      "Loss: 3.05609\n",
      "Time: 24.54 s\n",
      "--- Epoch  7/25 ---\n",
      "Loss: 2.95233\n",
      "Time: 25.41 s\n",
      "--- Epoch  8/25 ---\n",
      "Loss: 2.78189\n",
      "Time: 25.51 s\n",
      "--- Epoch  9/25 ---\n",
      "Loss: 2.62719\n",
      "Time: 25.78 s\n",
      "--- Epoch 10/25 ---\n",
      "Loss: 2.51607\n",
      "Time: 25.73 s\n",
      "--- Epoch 11/25 ---\n",
      "Loss: 2.44940\n",
      "Time: 26.22 s\n",
      "--- Epoch 12/25 ---\n",
      "Loss: 2.41050\n",
      "Time: 25.81 s\n",
      "--- Epoch 13/25 ---\n",
      "Loss: 2.35882\n",
      "Time: 24.66 s\n",
      "--- Epoch 14/25 ---\n",
      "Loss: 2.31868\n",
      "Time: 26.55 s\n",
      "--- Epoch 15/25 ---\n",
      "Loss: 2.28572\n",
      "Time: 26.55 s\n",
      "--- Epoch 16/25 ---\n",
      "Loss: 2.25638\n",
      "Time: 24.94 s\n",
      "--- Epoch 17/25 ---\n",
      "Loss: 2.22560\n",
      "Time: 26.19 s\n",
      "--- Epoch 18/25 ---\n",
      "Loss: 2.20141\n",
      "Time: 24.57 s\n",
      "--- Epoch 19/25 ---\n",
      "Loss: 2.16761\n",
      "Time: 25.89 s\n",
      "--- Epoch 20/25 ---\n",
      "Loss: 2.15345\n",
      "Time: 24.78 s\n",
      "--- Epoch 21/25 ---\n",
      "Loss: 2.12947\n",
      "Time: 25.87 s\n",
      "--- Epoch 22/25 ---\n",
      "Loss: 2.10990\n",
      "Time: 25.48 s\n",
      "--- Epoch 23/25 ---\n",
      "Loss: 2.08305\n",
      "Time: 24.76 s\n",
      "--- Epoch 24/25 ---\n",
      "Loss: 2.06400\n",
      "Time: 24.66 s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "len_sequence = 100\n",
    "epochs = 25\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, optimizer, loss_function, encoded, epochs=epochs, batch_size=batch_size, len_sequence=len_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make directory for models\n",
    "try:\n",
    "    os.mkdir(\"models\")\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "checkpoint = {\n",
    "    \"n_hidden\": model.n_hidden,\n",
    "    \"n_layers\": model.n_layers,\n",
    "    \"state_dict\": model.state_dict(),\n",
    "}\n",
    "        \n",
    "torch.save(checkpoint, \"models/CharLSTM.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(char, model, char2int, int2char, tokens, top_k=3, hidden=None,  device=device):\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Transform char to tokenized input\n",
    "    inputs = np.array([[char2int[char]]])\n",
    "    \n",
    "    # One-hot encode input\n",
    "    n_tokens = len(tokens)\n",
    "    inputs = one_hot_encoder(inputs, n_tokens)\n",
    "    \n",
    "    # Transform numpy array to torch tensor\n",
    "    inputs = torch.from_numpy(inputs).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        output, hidden = model(inputs, hidden)\n",
    "\n",
    "        # Get probabilities for next character\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        \n",
    "        # Get top characters\n",
    "        p, top_char = probabilities.topk(top_k)\n",
    "        top_char = top_char.cpu().numpy().squeeze()\n",
    "        \n",
    "        # Select next character amont top_k most probable\n",
    "        # Assign probabilities proportional to predicted probability\n",
    "        p = p.cpu().numpy().squeeze()\n",
    "        nextchar = np.random.choice(top_char, p=p/p.sum())\n",
    "        \n",
    "        # Return predicted char and hidden state\n",
    "        return int2char[nextchar], hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, length, prime, char2int, int2char, tokens, top_k=3, device=device):\n",
    "    \n",
    "    # List of prime characters\n",
    "    chars = list(prime)\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    hidden = None\n",
    "    \n",
    "    # Run on prime\n",
    "    for char in chars:\n",
    "        char, h = predict(char, model, char2int, int2char, tokens, hidden=hidden, top_k=top_k, device=device)\n",
    "\n",
    "    # Append first prediction after prime\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Use previous prediction to obtain a new prediction\n",
    "    for _ in range(length):\n",
    "        char, hidden = predict(chars[-1], model, char2int, int2char, tokens, hidden=hidden, top_k=top_k, device=device)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nel mezzo del cammin di nostra vitaro sonte,\n",
      "che l' meste a conto la saltore che più ch'io ch'io son che di conto, costere\n",
      "che 'n conton coner son li sante lia sente,\n",
      "che l'ancor di sue con lanso alleno\n",
      "come che l'artra al conta altra che lansa,\n",
      "seran le sola costio antrena\n",
      "serte, e con che su lia sera allere\n",
      "che 'l por le che sonto a piante,\n",
      "che se son la por ler più che la sante,\n",
      "e comina cose il sel sono alle concerto\n",
      "che sonte al son se sarto che sento\n",
      "comer le comenti i sol se lanso\n",
      "che l'olto e ciasciaro a par lera che possa\n",
      "di sa lia conter con li conto\n",
      "chi lo chi con son con la selte a conto,\n",
      "ch'io chi se la con conti conto la sono.\n",
      "\n",
      "Quanto a piùnto a la santonto\n",
      "de l'or con che so la ser so panso\n",
      "cose cie così che la core la pesta.\n",
      "\n",
      "La suo antron den la sento chi sondi,\n",
      "se più discia la porta, sì sonta,\n",
      "chi se l'anton de sen cossa costo\n",
      "sonte a la porte a con le sanconte.\n",
      "\n",
      "Questo a pense l'arto altre antro sero.\n",
      "\n",
      "E quallo a cii seno a la sonto sente.\n",
      "\n",
      "E di la sera a parte anto sorta\n",
      "sa cora le cien dantre la sera sera.\n",
      "\n",
      "La suan sol pertanto se la sero\n",
      "conen lia se le conto che posti,\n",
      "con comen l' mer che l'anco so cora\n",
      "deltra al selto, e ch'i' pia lia piosca,\n",
      "e che se sara se liartora ancora,\n",
      "se per ciestro cone comi al sera\n",
      "con che 'l porte sante le cosse serto\n",
      "ch'en sen se pare la piose a lanta,\n",
      "sì sende li serena a pen concerte.\n",
      "\n",
      "Quen l'i pore che li costo con le serto\n",
      "de che son li sol sonor den li sonco,\n",
      "che sere altrora al pera la cosco\n",
      "sanco l'antra a conte ser la sora,\n",
      "sì che 'l pionto al core a core ch'ia ch'ianto,\n",
      "so che l'arto sa perte li pente\n",
      "sa pione in con li casse sante,\n",
      "endo che serto anden ch'a che la conte\n",
      "sa che suonta a cor ser sa le conto\n",
      "ciò, che l'artra cieno a la sol sora\n",
      "con le che la per chier sento sento\n",
      "solante antora del sor sonto sarto\n",
      "de la per di con che li salana,\n",
      "con che sonte sartor li perete andono.\n",
      "\n",
      "Quan con la perto e la costi sonto..\n",
      "\n",
      "Quel la sua sonte li santo ch'ii conte..\n",
      "\n",
      "E questi al core antento e la cosco,\n",
      "e per ch'al cie lar sono el sonte li cort\n"
     ]
    }
   ],
   "source": [
    "generatedtext = sample(model, 2000, \"Nel mezzo del cammin di nostra vita\", char2int, int2char, chars)\n",
    "print(generatedtext)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
