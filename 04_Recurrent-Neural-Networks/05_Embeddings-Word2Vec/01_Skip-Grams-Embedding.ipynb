{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec word embedding using the skip-gram architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda\n"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "anarchism originated as a term of abuse first used against early working class radicals including t\n"
    }
   ],
   "source": [
    "# Load text\n",
    "with open(\"data/text8\") as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform punctuation into words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Replacing ! with  <EXCLAMATIONMARK> \nReplacing \" with  <QUOTATIONMARK> \nReplacing ( with  <LPAREN> \nReplacing ) with  <RPAREN> \nReplacing , with  <COMMA> \nReplacing . with  <PERIOD> \nReplacing : with  <COLON>\nReplacing ; with  <SEMICOLON> \nReplacing ? with  <QUESTIONMARK> \n"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation_names = {\n",
    "    \".\": \" <PERIOD> \",\n",
    "    \",\": \" <COMMA> \",\n",
    "    \"\\\"\": \" <QUOTATIONMARK> \", \n",
    "    \":\": \" <COLON> \", \n",
    "    \";\": \" <SEMICOLON> \", \n",
    "    \"!\": \" <EXCLAMATIONMARK> \", \n",
    "    \"?\": \" <QUESTIONMARK> \", \n",
    "    \"(\": \" <LPAREN> \", \n",
    "    \")\": \" <RPAREN> \",\n",
    "}\n",
    "\n",
    "text = text.lower()\n",
    "\n",
    "for p in punctuation:\n",
    "    try:\n",
    "        print(f\"Replacing {p} with {punctuation_names[p]}\")\n",
    "        text.replace(p, punctuation_names[p])\n",
    "    except KeyError:\n",
    "        # Remove punctuation\n",
    "        text.replace(p, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes']\n"
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(words[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "wordcounts = Counter(words)\n",
    "\n",
    "# Remove rare words\n",
    "words = [word for word in words if wordcounts[word] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total words: 16680599\nUnique words: 63641\n"
    }
   ],
   "source": [
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word = {i: word for i, word in enumerate(words)}\n",
    "word_to_int = {word: i for i, word in int_to_word.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now transform the whole text into integer numbers representing different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[14656338, 16667876, 16680491, 16680543, 16679204, 16680577, 16656223, 16679786, 16680352, 16680326, 16678804, 16675832, 16674838, 16555448, 16679417, 16680575, 14486722, 16680577, 16680575, 16673274, 16675992, 16680586, 16680575, 16659558, 12996717]\n"
    }
   ],
   "source": [
    "iwords = [word_to_int[word] for word in words]\n",
    "\n",
    "print(iwords[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrowing the Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very common words, usually do not provide useful context information (because they can be used in many different context). We can therefore subsample the dataset in order to obtain a more meaningful context reprsentation. Mikolov's subsampling consists in removing a word $w_i$ with probability $p_i$ given by\n",
    "\n",
    "$$\n",
    "    p_i(t) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\n",
    "$$\n",
    "\n",
    "where $t$ is a given threshold parameter and $f(w_i)$ is the frequency of word $w_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Threshold\n",
    "t = 1e-5\n",
    "\n",
    "n_iwords = len(iwords)\n",
    "\n",
    "iwordcounts = Counter(iwords)\n",
    "\n",
    "frequencies = {iword: count / n_iwords for iword, count in iwordcounts.items()}\n",
    "pdrop = {iword: 1 - np.sqrt(t / frequencies[iword]) for iword in iwords}\n",
    "\n",
    "trainset = [iword for iword in iwords if random.random() < (1 - pdrop[iword])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593523969377",
   "display_name": "Python 3.6.7 64-bit ('bertelsmann': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}